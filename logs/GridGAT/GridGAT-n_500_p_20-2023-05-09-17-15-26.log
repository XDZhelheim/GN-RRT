n_500_p_20
Trainset:	x-(7000, 20, 20, 3)	y-(7000, 20, 20, 1)
Valset:  	x-(999, 20, 20, 3)  	y-(999, 20, 20, 1)
Testset:	x-(2001, 20, 20, 3)	y-(2001, 20, 20, 1)

--------- GridGAT ---------
{
    "num_grids_height": 20,
    "num_grids_width": 20,
    "train_size": 0.7,
    "val_size": 0.1,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 32,
    "max_epochs": 200,
    "model_args": {
        "num_grids_height": 20,
        "num_grids_width": 20,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 32,
        "grid_embedding_dim": 32,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GridGAT                                  [32, 20, 20, 1]           12,800
├─Linear: 1-1                            [32, 400, 32]             128
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-1           [32, 400, 64]             --
│    │    └─AttentionLayer: 3-1          [32, 400, 64]             16,640
│    │    └─Dropout: 3-2                 [32, 400, 64]             --
│    │    └─LayerNorm: 3-3               [32, 400, 64]             128
│    │    └─Sequential: 3-4              [32, 400, 64]             16,576
│    │    └─Dropout: 3-5                 [32, 400, 64]             --
│    │    └─LayerNorm: 3-6               [32, 400, 64]             128
│    └─SelfAttentionLayer: 2-2           [32, 400, 64]             --
│    │    └─AttentionLayer: 3-7          [32, 400, 64]             16,640
│    │    └─Dropout: 3-8                 [32, 400, 64]             --
│    │    └─LayerNorm: 3-9               [32, 400, 64]             128
│    │    └─Sequential: 3-10             [32, 400, 64]             16,576
│    │    └─Dropout: 3-11                [32, 400, 64]             --
│    │    └─LayerNorm: 3-12              [32, 400, 64]             128
│    └─SelfAttentionLayer: 2-3           [32, 400, 64]             --
│    │    └─AttentionLayer: 3-13         [32, 400, 64]             16,640
│    │    └─Dropout: 3-14                [32, 400, 64]             --
│    │    └─LayerNorm: 3-15              [32, 400, 64]             128
│    │    └─Sequential: 3-16             [32, 400, 64]             16,576
│    │    └─Dropout: 3-17                [32, 400, 64]             --
│    │    └─LayerNorm: 3-18              [32, 400, 64]             128
├─Sequential: 1-3                        [32, 400, 1]              --
│    └─Linear: 2-4                       [32, 400, 64]             4,160
│    └─ReLU: 2-5                         [32, 400, 64]             --
│    └─Linear: 2-6                       [32, 400, 1]              65
==========================================================================================
Total params: 117,569
Trainable params: 117,569
Non-trainable params: 0
Total mult-adds (M): 3.35
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 186.88
Params size (MB): 0.42
Estimated Total Size (MB): 187.45
==========================================================================================

Loss: MaskedMAELoss

2023-05-09 17:15:33.364038 Epoch 1  	Train Loss = 0.06475 Val Loss = 0.06678
2023-05-09 17:15:37.325057 Epoch 2  	Train Loss = 0.06080 Val Loss = 0.06107
2023-05-09 17:15:41.185333 Epoch 3  	Train Loss = 0.06012 Val Loss = 0.06079
2023-05-09 17:15:45.001640 Epoch 4  	Train Loss = 0.05963 Val Loss = 0.06078
2023-05-09 17:15:48.984015 Epoch 5  	Train Loss = 0.05972 Val Loss = 0.06108
2023-05-09 17:15:52.894103 Epoch 6  	Train Loss = 0.05942 Val Loss = 0.06096
2023-05-09 17:15:56.860733 Epoch 7  	Train Loss = 0.05986 Val Loss = 0.06104
2023-05-09 17:16:00.621467 Epoch 8  	Train Loss = 0.05953 Val Loss = 0.06279
2023-05-09 17:16:04.613634 Epoch 9  	Train Loss = 0.05896 Val Loss = 0.06038
2023-05-09 17:16:08.626804 Epoch 10  	Train Loss = 0.05877 Val Loss = 0.05990
2023-05-09 17:16:12.562979 Epoch 11  	Train Loss = 0.05811 Val Loss = 0.06011
2023-05-09 17:16:16.105630 Epoch 12  	Train Loss = 0.05807 Val Loss = 0.06003
2023-05-09 17:16:20.034708 Epoch 13  	Train Loss = 0.05803 Val Loss = 0.06014
2023-05-09 17:16:23.765455 Epoch 14  	Train Loss = 0.05821 Val Loss = 0.06015
2023-05-09 17:16:27.644873 Epoch 15  	Train Loss = 0.05798 Val Loss = 0.06009
2023-05-09 17:16:31.476380 Epoch 16  	Train Loss = 0.05807 Val Loss = 0.06021
2023-05-09 17:16:35.356094 Epoch 17  	Train Loss = 0.05805 Val Loss = 0.06009
2023-05-09 17:16:39.097504 Epoch 18  	Train Loss = 0.05812 Val Loss = 0.06008
2023-05-09 17:16:42.812203 Epoch 19  	Train Loss = 0.05807 Val Loss = 0.06007
2023-05-09 17:16:46.711670 Epoch 20  	Train Loss = 0.05801 Val Loss = 0.06016
Early stopping at epoch: 20
Best at epoch 10:
Train Loss = 0.05877
Train MAE = 0.05764, RMSE = 0.08543
Val Loss = 0.05990
Val MAE = 0.05919, RMSE = 0.08886
--------- Test ---------
Test MAE = 0.05800, RMSE = 0.08466
Inference time: 0.26 s
