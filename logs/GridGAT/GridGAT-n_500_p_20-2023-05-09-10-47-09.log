n_500_p_20
Trainset:	x-(7000, 20, 20, 3)	y-(7000, 20, 20, 1)
Valset:  	x-(999, 20, 20, 3)  	y-(999, 20, 20, 1)
Testset:	x-(2001, 20, 20, 3)	y-(2001, 20, 20, 1)

--------- GridGAT ---------
{
    "num_grids_height": 20,
    "num_grids_width": 20,
    "train_size": 0.7,
    "val_size": 0.1,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 32,
    "max_epochs": 200,
    "model_args": {
        "num_grids_height": 20,
        "num_grids_width": 20,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 32,
        "grid_embedding_dim": 32,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GridGAT                                  [32, 20, 20, 1]           12,800
├─Linear: 1-1                            [32, 400, 32]             128
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-1           [32, 400, 64]             --
│    │    └─AttentionLayer: 3-1          [32, 400, 64]             16,640
│    │    └─Dropout: 3-2                 [32, 400, 64]             --
│    │    └─LayerNorm: 3-3               [32, 400, 64]             128
│    │    └─Sequential: 3-4              [32, 400, 64]             16,576
│    │    └─Dropout: 3-5                 [32, 400, 64]             --
│    │    └─LayerNorm: 3-6               [32, 400, 64]             128
│    └─SelfAttentionLayer: 2-2           [32, 400, 64]             --
│    │    └─AttentionLayer: 3-7          [32, 400, 64]             16,640
│    │    └─Dropout: 3-8                 [32, 400, 64]             --
│    │    └─LayerNorm: 3-9               [32, 400, 64]             128
│    │    └─Sequential: 3-10             [32, 400, 64]             16,576
│    │    └─Dropout: 3-11                [32, 400, 64]             --
│    │    └─LayerNorm: 3-12              [32, 400, 64]             128
│    └─SelfAttentionLayer: 2-3           [32, 400, 64]             --
│    │    └─AttentionLayer: 3-13         [32, 400, 64]             16,640
│    │    └─Dropout: 3-14                [32, 400, 64]             --
│    │    └─LayerNorm: 3-15              [32, 400, 64]             128
│    │    └─Sequential: 3-16             [32, 400, 64]             16,576
│    │    └─Dropout: 3-17                [32, 400, 64]             --
│    │    └─LayerNorm: 3-18              [32, 400, 64]             128
├─Sequential: 1-3                        [32, 400, 1]              --
│    └─Linear: 2-4                       [32, 400, 64]             4,160
│    └─ReLU: 2-5                         [32, 400, 64]             --
│    └─Linear: 2-6                       [32, 400, 1]              65
==========================================================================================
Total params: 117,569
Trainable params: 117,569
Non-trainable params: 0
Total mult-adds (M): 3.35
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 186.88
Params size (MB): 0.42
Estimated Total Size (MB): 187.45
==========================================================================================

Loss: MaskedMAELoss

2023-05-09 10:47:19.105043 Epoch 1  	Train Loss = 0.08931 Val Loss = 0.08743
2023-05-09 10:47:24.995220 Epoch 2  	Train Loss = 0.08498 Val Loss = 0.08815
2023-05-09 10:47:30.897597 Epoch 3  	Train Loss = 0.08405 Val Loss = 0.08387
2023-05-09 10:47:36.783839 Epoch 4  	Train Loss = 0.08188 Val Loss = 0.08394
2023-05-09 10:47:42.660550 Epoch 5  	Train Loss = 0.08207 Val Loss = 0.08454
2023-05-09 10:47:48.521590 Epoch 6  	Train Loss = 0.08116 Val Loss = 0.08277
2023-05-09 10:47:54.370355 Epoch 7  	Train Loss = 0.08045 Val Loss = 0.08173
2023-05-09 10:48:00.251415 Epoch 8  	Train Loss = 0.07884 Val Loss = 0.08178
2023-05-09 10:48:06.138746 Epoch 9  	Train Loss = 0.07814 Val Loss = 0.07970
2023-05-09 10:48:11.998761 Epoch 10  	Train Loss = 0.07771 Val Loss = 0.07985
2023-05-09 10:48:17.867797 Epoch 11  	Train Loss = 0.07622 Val Loss = 0.07885
2023-05-09 10:48:23.738386 Epoch 12  	Train Loss = 0.07589 Val Loss = 0.07884
2023-05-09 10:48:29.608658 Epoch 13  	Train Loss = 0.07567 Val Loss = 0.07850
2023-05-09 10:48:35.518842 Epoch 14  	Train Loss = 0.07584 Val Loss = 0.07835
2023-05-09 10:48:41.413481 Epoch 15  	Train Loss = 0.07537 Val Loss = 0.07820
2023-05-09 10:48:47.271981 Epoch 16  	Train Loss = 0.07544 Val Loss = 0.07825
2023-05-09 10:48:51.847082 Epoch 17  	Train Loss = 0.07526 Val Loss = 0.07839
2023-05-09 10:48:55.642222 Epoch 18  	Train Loss = 0.07525 Val Loss = 0.07808
2023-05-09 10:48:59.440050 Epoch 19  	Train Loss = 0.07513 Val Loss = 0.07813
2023-05-09 10:49:03.369059 Epoch 20  	Train Loss = 0.07499 Val Loss = 0.07805
2023-05-09 10:49:07.193827 Epoch 21  	Train Loss = 0.07502 Val Loss = 0.07802
2023-05-09 10:49:10.979390 Epoch 22  	Train Loss = 0.07491 Val Loss = 0.07841
2023-05-09 10:49:14.961874 Epoch 23  	Train Loss = 0.07480 Val Loss = 0.07818
2023-05-09 10:49:18.981915 Epoch 24  	Train Loss = 0.07478 Val Loss = 0.07786
2023-05-09 10:49:22.866656 Epoch 25  	Train Loss = 0.07463 Val Loss = 0.07784
2023-05-09 10:49:26.747547 Epoch 26  	Train Loss = 0.07449 Val Loss = 0.07777
2023-05-09 10:49:30.675651 Epoch 27  	Train Loss = 0.07456 Val Loss = 0.07775
2023-05-09 10:49:34.598145 Epoch 28  	Train Loss = 0.07453 Val Loss = 0.07779
2023-05-09 10:49:38.488539 Epoch 29  	Train Loss = 0.07443 Val Loss = 0.07768
2023-05-09 10:49:42.431763 Epoch 30  	Train Loss = 0.07427 Val Loss = 0.07812
2023-05-09 10:49:46.416465 Epoch 31  	Train Loss = 0.07399 Val Loss = 0.07764
2023-05-09 10:49:50.430290 Epoch 32  	Train Loss = 0.07385 Val Loss = 0.07765
2023-05-09 10:49:54.355543 Epoch 33  	Train Loss = 0.07398 Val Loss = 0.07764
2023-05-09 10:49:58.215204 Epoch 34  	Train Loss = 0.07388 Val Loss = 0.07768
2023-05-09 10:50:02.157717 Epoch 35  	Train Loss = 0.07381 Val Loss = 0.07767
2023-05-09 10:50:06.192280 Epoch 36  	Train Loss = 0.07373 Val Loss = 0.07768
2023-05-09 10:50:10.062393 Epoch 37  	Train Loss = 0.07390 Val Loss = 0.07766
2023-05-09 10:50:14.065157 Epoch 38  	Train Loss = 0.07389 Val Loss = 0.07765
2023-05-09 10:50:18.130981 Epoch 39  	Train Loss = 0.07365 Val Loss = 0.07768
2023-05-09 10:50:21.961386 Epoch 40  	Train Loss = 0.07374 Val Loss = 0.07765
2023-05-09 10:50:25.838120 Epoch 41  	Train Loss = 0.07373 Val Loss = 0.07766
Early stopping at epoch: 41
Best at epoch 31:
Train Loss = 0.07399
Train MAE = 0.07320, RMSE = 0.11369
Val Loss = 0.07764
Val MAE = 0.07582, RMSE = 0.11774
--------- Test ---------
Test MAE = 0.07556, RMSE = 0.11626
Inference time: 0.26 s
